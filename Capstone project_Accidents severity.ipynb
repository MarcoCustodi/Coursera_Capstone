{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Car Accident Severity in Seattle**\n",
    "## Applied Data Science Capstone - Coursera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: This notebook shows the process of building a machine learning model for accident severity prediction. It is part of the final capstone project in Coursera to obtain the IBM Professional Certificate in Data Science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Car accidents happen every day for a variety of reasons and these have significant socioeconomic costs. Efforts to raise drivers' awareness towards mindful driving have been promoted across the USA and the authorities try to provide the conditions (e.g. road signs, traffic lights, traffic information, radars) to mitigate the probability of accidents happening. \n",
    "Today we have the data and the modeling capacities to even better understand the conditions that promote severe accidents and this project intends to build a machine learning model to better inform decision-makers in the city of Seattle using available data. This model will help the authorities to take appropriate measures to reduce accident severity and improve traffic safety.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(r'C:\\Users\\marco\\Desktop\\Data Science\\IBM Coursera\\Capstone project\\Data-Collisions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was provided by the Seattle Police Department and corresponds to collisions registered between 2004 and 2020. The data is stored in a CSV file, presenting 38 columns and 194673 rows. It describes the details of each accident, including weather conditions, collision type, date/time of accident and location.\n",
    "\n",
    "In the dataset we have 3 types of variables: integers (12), floats (4) and objects (22), as we can see below:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable SEVERITYCODE encodes the Seattle Department of Transport accident severity metric and this will be our 'dependent variable' (the variable we want to predict). The numerical codes and their meaning are as follows:\n",
    "\n",
    "* 0: Unknown\n",
    "* 1: Property damage\n",
    "* 2: Injury\n",
    "* 2b: Serious injury\n",
    "* 3: Fatality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SEVERITYCODE'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By analysing the dataset, we can see that there are only two levels (out of five) of 'severity' registered:\n",
    "- 1: 136485 registrations\n",
    "- 2: 58188 registrations\n",
    "\n",
    "The data is unbalanced, since we have many more instances of 'severity 1' compared with 'severity 2'. Data must be balanced and normalized in the data processsing step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 37 attributes (columns) that can be used for building the model , but not all are useful.\n",
    "\n",
    "At this stage, the following columns were dropped from the dataset as they were deemed not useful for the model (e.g. unnecessary, uninformative or redundant columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "editable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['JUNCTIONTYPE','INCDATE','PEDROWNOTGRNT','ST_COLCODE','PEDCYLCOUNT','PERSONCOUNT','PEDCOUNT','COLLISIONTYPE', 'OBJECTID', 'X', 'Y', 'INCKEY', 'REPORTNO', 'EXCEPTRSNCODE', 'EXCEPTRSNDESC', 'SEVERITYCODE.1', 'SEVERITYDESC', 'STATUS', 'COLDETKEY', 'LOCATION', 'INTKEY', 'INCDTTM','SDOT_COLDESC', 'SDOT_COLCODE', 'INATTENTIONIND', 'SDOTCOLNUM', 'ST_COLDESC', 'SEGLANEKEY', 'CROSSWALKKEY'], inplace= True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Methodology**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will perform a data wrangling step to prepare the dataset for analysis. First lets look at missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking for missing data\n",
    "missing_data = df.isnull()\n",
    "for column in missing_data.columns.values.tolist():\n",
    "    print(column)\n",
    "    print (missing_data[column].value_counts())\n",
    "    print(\"---------------------\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output above, we can see that columns 'SPEEDING' has much more missing data cells than not. Therefore we will drop this columns from our analysis. Columns 'ADDRTYPE', 'JUNCTIONTYPE', 'UNDERINFL', 'WEATHER', 'ROADCOND' and 'LIGHTCOND' have some missing data. In this case, we will drop the rows with missing values on those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['SPEEDING'], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna(axis=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the column 'UNDERINFL' we have a mix of numerical (0 and 1) and categorical data (Y and N). We will convert Y to 1 and N to 0 to uniformize the data to numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['UNDERINFL'].replace('Y', 1, inplace=True)\n",
    "df['UNDERINFL'].replace('N', 0, inplace=True)\n",
    "df['UNDERINFL']= df['UNDERINFL'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also convert 'HITPARKEDCAR' to numeric variables 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['HITPARKEDCAR'].replace('Y', 1, inplace=True)\n",
    "df['HITPARKEDCAR'].replace('N', 0, inplace=True)\n",
    "df['HITPARKEDCAR']= df['HITPARKEDCAR'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, in column 'LIGHTCOND' we will merge the categorie levels 'Dark - No Street Lights' and 'Dark - Street Lights Off' to a single category 'Dark - Street Light Off'. We will eliminate rows with 'Other' and 'Dark - Unknown Lighting' as they represent a small fraction of cases and do not provide relevant information. Let first look at value counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LIGHTCOND'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LIGHTCOND'].replace('Dark - No Street Lights', 'Night', inplace=True)\n",
    "df['LIGHTCOND'].replace('Dusk', 'Dusk/Dawn', inplace=True)\n",
    "df['LIGHTCOND'].replace('Dawn', 'Dusk/Dawn', inplace=True)\n",
    "df['LIGHTCOND'].replace('Dark - Street Lights On', 'Night', inplace=True)\n",
    "df['LIGHTCOND'].replace('Dark - Street Lights Off', 'Night', inplace=True)\n",
    "indexNames = df[df['LIGHTCOND'] == 'Other'].index\n",
    "df.drop(indexNames, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexNames = df[df['LIGHTCOND'] == 'Dark - Unknown Lighting'].index\n",
    "df.drop(indexNames, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexNames = df[df['LIGHTCOND'] == 'Unknown'].index\n",
    "df.drop(indexNames, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns 'WEATHER' will also have merged and dropped categories to reduce category levels. New category 'Elements' will include all categories that involve weather elements like rain, snow and wind. We will drop 'Partly Cloudy', 'Unknown' and 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['WEATHER'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['WEATHER'].replace('Raining', 'Elements', inplace=True)\n",
    "df['WEATHER'].replace('Snowing', 'Elements', inplace=True)\n",
    "df['WEATHER'].replace('Sleet/Hail/Freezing Rain', 'Elements', inplace=True)\n",
    "df['WEATHER'].replace('Raining', 'Elements', inplace=True)\n",
    "df['WEATHER'].replace('Fog/Smog/Smoke', 'Elements', inplace=True)\n",
    "df['WEATHER'].replace('Blowing Sand/Dirt', 'Elements', inplace=True)\n",
    "df['WEATHER'].replace('Severe Crosswind', 'Elements', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexNames = df[df['WEATHER'] == 'Other'].index\n",
    "df.drop(indexNames, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexNames = df[df['WEATHER'] == 'Unknown'].index\n",
    "df.drop(indexNames, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexNames = df[df['WEATHER'] == 'Partly Cloudy'].index\n",
    "df.drop(indexNames, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns 'ROADCOND' will also have merged and dropped categories to reduce category levels. New category 'Elements' will include all categories that involve elements like water, sand and ice. We will drop 'Unknown' and 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ROADCOND'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ROADCOND'].replace('Ice', 'Elements', inplace=True)\n",
    "df['ROADCOND'].replace('Wet', 'Elements', inplace=True)\n",
    "df['ROADCOND'].replace('Snow/Slush', 'Elements', inplace=True)\n",
    "df['ROADCOND'].replace('Standing Water', 'Elements', inplace=True)\n",
    "df['ROADCOND'].replace('Sand/Mud/Dirt', 'Elements', inplace=True)\n",
    "df['ROADCOND'].replace('Oil', 'Elements', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexNames = df[df['ROADCOND'] == 'Other'].index\n",
    "df.drop(indexNames, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexNames = df[df['ROADCOND'] == 'Unknown'].index\n",
    "df.drop(indexNames, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw above, 'SEVERITYCODE' (the dependent varaiable we want to predict based on selected features) is unbalanced, since we have many more cells with severity code '1' than '2'. Let's use resampling to balance the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SEVERITYCODE'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "df_sevcode_1= df[df[\"SEVERITYCODE\"]== 1]\n",
    "df_sevcode_2= df[df[\"SEVERITYCODE\"]== 2]\n",
    "df_sevcode_1_down= resample(df_sevcode_1,\n",
    "                           replace= False,\n",
    "                           n_samples= 55536,\n",
    "                           random_state= 123)\n",
    "df= pd.concat([df_sevcode_1_down, df_sevcode_2])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SEVERITYCODE'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most Machine Learning models require numerical data, we need to convert categorical variables to numerical ones. We will us one-hot enconding for this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.get_dummies(df[[\"ADDRTYPE\"]])\n",
    "df3=pd.get_dummies(df[[\"WEATHER\"]])\n",
    "df4=pd.get_dummies(df[[\"ROADCOND\"]])\n",
    "df5=pd.get_dummies(df[[\"LIGHTCOND\"]])\n",
    "df=pd.concat([df,df2,df3,df4,df5],axis=1)\n",
    "df.drop(columns=[\"ADDRTYPE\", \"WEATHER\", \"ROADCOND\", \"LIGHTCOND\"], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now remove the columns with an 'Unknown' label since these do not provide addtional information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data will be now standardised (i.e. every column re-scaled to have ~zero mean and ~unit variance) using the scikit learn StandardScaler routine. But first let's define the feature set(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SEVERITYCODE']= df['SEVERITYCODE'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['VEHCOUNT', 'UNDERINFL','HITPARKEDCAR','ADDRTYPE_Alley','ADDRTYPE_Block', 'ADDRTYPE_Intersection','WEATHER_Clear',\n",
    "            'WEATHER_Elements','WEATHER_Overcast', 'ROADCOND_Dry','ROADCOND_Elements','LIGHTCOND_Daylight','LIGHTCOND_Night',\n",
    "            'LIGHTCOND_Dusk/Dawn']].values.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "X1 = preprocessing.StandardScaler().fit(X1).transform(X1.astype(float))\n",
    "X1[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['SEVERITYCODE'].values\n",
    "y[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.3, random_state=4)\n",
    "print ('Train set:', X_train.shape,  y_train.shape)\n",
    "print ('Test set:', X_test.shape,  y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1- k-Nearest Neighbours (kNN) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "Ks = 10\n",
    "mean_acc = np.zeros((Ks-1))\n",
    "std_acc = np.zeros((Ks-1))\n",
    "ConfustionMx = [];\n",
    "for n in range(1,Ks):\n",
    "    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n",
    "    KNNyhat=neigh.predict(X_test)\n",
    "    mean_acc[n-1] = metrics.accuracy_score(y_test, KNNyhat)\n",
    "\n",
    "    std_acc[n-1]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0])\n",
    "\n",
    "mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 8 \n",
    "neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\n",
    "neigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNNyhat = neigh.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(KNNyhat [0:5])\n",
    "print(y_test [0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test set Accuracy: \", metrics.accuracy_score(y_test, KNNyhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2- Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "severityTree = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4)\n",
    "severityTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severityTree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "DTyhat = severityTree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (DTyhat [0:5])\n",
    "print (y_test [0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"DecisionTrees's Accuracy: \", metrics.accuracy_score(y_test, DTyhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Support Vector Machine (SVM) model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='rbf')\n",
    "clf.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVMyhat = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (SVMyhat [0:5])\n",
    "print (y_test [0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SVM Accuracy: \", metrics.accuracy_score(y_test, SVMyhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_coefficients(classifier, feature_names, top_features=20):\n",
    "    coef = svm.coef_.ravel()\n",
    "    top_positive_coefficients = np.argsort(coef)[-top_features:]\n",
    "    top_negative_coefficients = np.argsort(coef)[:top_features]\n",
    "    top_coefficients = np.hstack([top_negative_coefficients, top_positive_coefficients])\n",
    "\n",
    "# create plot\n",
    "plt.figure(figsize=(15, 5))\n",
    "colors = [‘red’ if c < 0 else ‘blue’ for c in coef[top_coefficients]]\n",
    "plt.bar(np.arange(2 * top_features), coef[top_coefficients], color=colors)\n",
    "feature_names = np.array(feature_names)\n",
    "plt.xticks(np.arange(1, 1 + 2 * top_features), feature_names[top_coefficients], rotation=60, ha=’right’)\n",
    "plt.show()cv = CountVectorizer()\n",
    "cv.fit(data)\n",
    "print len(cv.vocabulary_)\n",
    "print cv.get_feature_names()X_train = cv.transform(data)\n",
    "\n",
    "svm = LinearSVC()\n",
    "svm.fit(X_train, target)plot_coefficients(svm, cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "import itertools\n",
    "\n",
    "print('Jaccard Similarity Score:')\n",
    "print('')\n",
    "print('KNN model:', jaccard_similarity_score(y_test, KNNyhat))\n",
    "print('Decision Tree model:', jaccard_similarity_score(y_test, DTyhat))\n",
    "print('SVM model:', jaccard_similarity_score(y_test, SVMyhat))\n",
    "print('-------------------')\n",
    "print('F1 Score')\n",
    "print('')\n",
    "print('KNN model:', f1_score(y_test, KNNyhat, average='weighted'))\n",
    "print('Decision Tree model:', f1_score(y_test, DTyhat, average='weighted'))\n",
    "print('SVM model:', f1_score(y_test, SVMyhat, average='weighted'))\n",
    "print('-------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Results and Discussion**\n",
    "\n",
    "In order to develop a model for predicting accident severity, the re-sampled, cleaned dataset was split in to testing and training sub-samples (containing 30% and 70% of the samples, respectively) using the scikit learn “train_test_split” method. In total, 3 models were trained and evaluated.\n",
    "\n",
    "### KNN model \n",
    "\n",
    "The value of'k' was established by running kNN models for k=1–10 using the kNeighborsClassifier function from scikit learn. The model is optimised at k=8, at which the model correctly predicts accident severity 61% of the time. The Jaccard Index and F1 score are respectively 0.614 and 0.612.\n",
    "\n",
    "### Decision Tree model \n",
    "\n",
    "A decision tree model was trained on the data according to the “entropy” criterion, and allowed to run until covergence. The decision tree correctly predicts accident severity 63% of the time and has Jaccard Index and F1 scores of 0.627 and 0.626 respectively.\n",
    "\n",
    "### SVM model \n",
    "\n",
    "An SVM model was built using the scikit learn C-Support Vector Classification method (svm.svc), with a linear mapping kernel employed in order that the model could return a list of the features with the most diagnostic power for determining accident severity. The SVM model correctly predicts accident severity 63% of the time, and has Jaccard Index and F1 scores of 0.628 and 0.627 respectively.\n",
    "\n",
    "From the model evaluation indexes, we can conclude that the 3 models have a similar capacity to predict accident severity.\n",
    "Models could be further explored by changing the features set and see if prediction accuracy increases by removing or including features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Conclusion**\n",
    "\n",
    "Car accident data for the city of Seattle between 2004–2019 have been used to train and evaluate machine learning models for predicting accident severity based on the context of the accident. Three classes of models have been trained and evaluated: (i) k-Nearest Neighbors, (ii) Decision Tree and (iii) Support Vector Machine. The three models performed similarly, predicting correctly 62-63% of severity scores, with a slightly better performance by the SVM model. This work highlights that machine learning techniques can be used to probe historical data in order to make reliable predictions about the outcome of road traffic accidents, given information which is available at the time when an accident is reported. This model can be extended to include new features and can give city planners insight into the road conditions/features which are associated with higher accident severity. By predicting accident severity as a function of weather, date, location and road conditions, this model may be able to help aid the decision making of city roads planning.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
